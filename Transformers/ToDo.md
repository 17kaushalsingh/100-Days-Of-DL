# Part 1
~~1. Introduction to Transformers~~
~~2. What is Self Attention~~
~~3. Self Attention in Transformers~~
~~4. Scaled Dot Product Attention~~
5. Geometric Intuition of Self Attention
6. Why is Self Attention Called Self
7. Multihead Attention in Transformers
8. Positional Encoding in Transformers
9. Layer Normalization in Transformers

# Part 2
10. Transformer Architecture: Encoder
11. Transformer Architecture: Masked Multihead Attention
12. Transformer Architecture: Cross Attention
13. Transformer Architecture: Decoder Architecture
14. Transformer Architecture: Inferences

# Revision
