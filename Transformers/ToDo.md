# Part 1
### 1. Multihead Attention in Transformers
### 2. Positional Encoding in Transformers
### 3. Layer Normalization in Transformers

# Part 2
### 1. Transformer Architecture: Encoder
### 2. Transformer Architecture: Masked Multihead Attention
### 3. Transformer Architecture: Cross Attention
### 4. Transformer Architecture: Decoder Architecture
### 5. Transformer Architecture: Inferences
