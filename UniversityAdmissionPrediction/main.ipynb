{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         500 non-null    int64  \n",
      " 1   GRE Score          500 non-null    int64  \n",
      " 2   TOEFL Score        500 non-null    int64  \n",
      " 3   University Rating  500 non-null    int64  \n",
      " 4   SOP                500 non-null    float64\n",
      " 5   LOR                500 non-null    float64\n",
      " 6   CGPA               500 non-null    float64\n",
      " 7   Research           500 non-null    int64  \n",
      " 8   Chance of Admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Serial No.', 'GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA', 'Research', 'Chance of Admit ']\n"
     ]
    }
   ],
   "source": [
    "cols = df.columns\n",
    "cols = cols.to_list()\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Serial No.'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Chance of Admit '])\n",
    "y = df['Chance of Admit ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0        337          118                  4  4.5   4.5  9.65         1\n",
       "1        324          107                  4  4.0   4.5  8.87         1\n",
       "2        316          104                  3  3.0   3.5  8.00         1\n",
       "3        322          110                  3  3.5   2.5  8.67         1\n",
       "4        314          103                  2  2.0   3.0  8.21         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.92\n",
       "1    0.76\n",
       "2    0.72\n",
       "3    0.80\n",
       "4    0.65\n",
       "Name: Chance of Admit , dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.72      ,  0.85714286,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.75241158,  1.        ],\n",
       "       [ 0.84      ,  0.92857143,  1.        ,  1.        ,  1.        ,\n",
       "         0.8585209 ,  1.        ],\n",
       "       [ 0.54      ,  0.42857143,  0.25      ,  0.875     ,  0.71428571,\n",
       "         0.53697749,  0.        ],\n",
       "       [ 0.4       ,  0.25      ,  0.25      ,  0.125     ,  0.14285714,\n",
       "         0.1607717 ,  0.        ],\n",
       "       [ 0.7       ,  0.78571429,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.72025723,  1.        ],\n",
       "       [ 0.78      ,  0.96428571,  0.75      ,  0.875     ,  0.85714286,\n",
       "         0.75884244,  1.        ],\n",
       "       [ 0.74      ,  0.75      ,  0.75      ,  0.875     ,  0.85714286,\n",
       "         0.74276527,  1.        ],\n",
       "       [ 0.42      ,  0.35714286,  0.5       ,  0.875     ,  0.71428571,\n",
       "         0.59163987,  1.        ],\n",
       "       [ 0.16      ,  0.21428571,  0.25      ,  0.75      ,  0.42857143,\n",
       "         0.39549839,  0.        ],\n",
       "       [ 0.34      ,  0.46428571,  0.75      ,  0.5       ,  0.42857143,\n",
       "         0.36655949,  0.        ],\n",
       "       [ 0.6       ,  0.32142857,  0.25      ,  0.375     ,  0.42857143,\n",
       "         0.585209  ,  0.        ],\n",
       "       [ 0.18      ,  0.28571429,  0.5       ,  0.25      ,  0.14285714,\n",
       "         0.39228296,  0.        ],\n",
       "       [ 0.5       ,  0.28571429,  0.        ,  0.25      ,  0.28571429,\n",
       "         0.36977492,  0.        ],\n",
       "       [ 0.12      ,  0.32142857,  0.        ,  0.375     ,  0.42857143,\n",
       "         0.2829582 ,  0.        ],\n",
       "       [ 0.54      ,  0.39285714,  0.25      ,  0.375     ,  0.14285714,\n",
       "         0.4340836 ,  0.        ],\n",
       "       [ 0.7       ,  0.64285714,  0.25      ,  0.5       ,  0.28571429,\n",
       "         0.63022508,  1.        ],\n",
       "       [ 0.82      ,  0.85714286,  1.        ,  0.75      ,  0.71428571,\n",
       "         0.79099678,  1.        ],\n",
       "       [ 0.74      ,  0.71428571,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.61736334,  1.        ],\n",
       "       [ 0.72      ,  0.78571429,  0.75      ,  0.75      ,  0.57142857,\n",
       "         0.75884244,  1.        ],\n",
       "       [ 0.7       ,  0.78571429,  1.        ,  0.75      ,  1.        ,\n",
       "         0.85530547,  1.        ],\n",
       "       [ 0.84      ,  0.89285714,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.73954984,  0.        ],\n",
       "       [ 0.66      ,  0.64285714,  0.5       ,  0.75      ,  0.57142857,\n",
       "         0.73954984,  1.        ],\n",
       "       [ 0.78      ,  0.75      ,  1.        ,  1.        ,  0.85714286,\n",
       "         0.85209003,  1.        ],\n",
       "       [ 0.68      ,  0.67857143,  0.5       ,  0.375     ,  0.        ,\n",
       "         0.63987138,  1.        ],\n",
       "       [ 0.56      ,  0.28571429,  0.25      ,  0.375     ,  0.57142857,\n",
       "         0.55948553,  1.        ],\n",
       "       [ 0.36      ,  0.39285714,  0.25      ,  0.375     ,  0.71428571,\n",
       "         0.50160772,  1.        ],\n",
       "       [ 0.16      ,  0.17857143,  0.25      ,  0.25      ,  0.42857143,\n",
       "         0.1318328 ,  0.        ],\n",
       "       [ 0.2       ,  0.35714286,  0.25      ,  0.125     ,  0.14285714,\n",
       "         0.34405145,  0.        ],\n",
       "       [ 0.18      ,  0.07142857,  0.        ,  0.        , -0.14285714,\n",
       "         0.17363344,  0.        ],\n",
       "       [ 0.66      ,  0.53571429,  0.5       ,  0.625     ,  0.57142857,\n",
       "         0.56270096,  1.        ],\n",
       "       [ 0.48      ,  0.42857143,  0.75      ,  1.        ,  1.        ,\n",
       "         0.71382637,  0.        ],\n",
       "       [ 0.58      ,  0.57142857,  0.5       ,  0.5       ,  0.57142857,\n",
       "         0.55948553,  1.        ],\n",
       "       [ 0.7       ,  0.5       ,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.51446945,  1.        ],\n",
       "       [ 0.68      ,  0.75      ,  1.        ,  0.75      ,  1.        ,\n",
       "         0.78778135,  1.        ],\n",
       "       [ 0.3       ,  0.39285714,  0.25      ,  0.375     ,  0.57142857,\n",
       "         0.42765273,  0.        ],\n",
       "       [ 0.76      ,  0.82142857,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.75884244,  1.        ],\n",
       "       [ 0.3       ,  0.14285714,  0.75      ,  0.5       ,  0.85714286,\n",
       "         0.46945338,  0.        ],\n",
       "       [ 0.6       ,  0.64285714,  1.        ,  0.75      ,  0.71428571,\n",
       "         0.79421222,  1.        ],\n",
       "       [ 0.16      ,  0.25      ,  0.        ,  0.125     ,  0.42857143,\n",
       "         0.21221865,  0.        ],\n",
       "       [ 0.18      ,  0.28571429,  0.5       ,  0.25      ,  0.42857143,\n",
       "         0.19935691,  0.        ],\n",
       "       [ 0.44      ,  0.28571429,  0.        ,  0.5       ,  0.42857143,\n",
       "         0.5562701 ,  1.        ],\n",
       "       [ 0.92      ,  0.96428571,  1.        ,  0.75      ,  0.57142857,\n",
       "         0.96463023,  1.        ],\n",
       "       [ 0.66      ,  0.67857143,  1.        ,  0.75      ,  1.        ,\n",
       "         0.98392283,  1.        ],\n",
       "       [ 0.98      ,  0.85714286,  0.75      ,  0.75      ,  0.57142857,\n",
       "         0.96463023,  1.        ],\n",
       "       [ 0.74      ,  0.78571429,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.71382637,  0.        ],\n",
       "       [ 0.3       ,  0.35714286,  0.25      ,  0.25      ,  0.28571429,\n",
       "         0.4437299 ,  0.        ],\n",
       "       [ 0.52      ,  0.60714286,  0.75      ,  0.875     ,  0.57142857,\n",
       "         0.63022508,  1.        ],\n",
       "       [ 0.68      ,  0.64285714,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.75562701,  1.        ],\n",
       "       [ 0.82      ,  0.71428571,  1.        ,  0.75      ,  1.        ,\n",
       "         0.96463023,  1.        ],\n",
       "       [ 0.38      ,  0.25      ,  0.5       ,  0.75      ,  0.71428571,\n",
       "         0.5659164 ,  0.        ],\n",
       "       [ 0.28      ,  0.28571429,  0.25      ,  0.625     ,  0.42857143,\n",
       "         0.45659164,  0.        ],\n",
       "       [ 0.6       ,  0.39285714,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.28938907,  0.        ],\n",
       "       [ 0.48      ,  0.57142857,  0.5       ,  0.875     ,  0.57142857,\n",
       "         0.43086817,  0.        ],\n",
       "       [ 0.22      ,  0.21428571,  0.        ,  0.25      ,  0.42857143,\n",
       "         0.39549839,  1.        ],\n",
       "       [ 1.        ,  1.        ,  1.        ,  0.875     ,  0.85714286,\n",
       "         0.90032154,  1.        ],\n",
       "       [ 0.68      ,  0.28571429,  0.5       ,  0.75      ,  1.        ,\n",
       "         0.59163987,  1.        ],\n",
       "       [ 0.84      ,  0.71428571,  0.        ,  0.125     ,  0.42857143,\n",
       "         0.59807074,  1.        ],\n",
       "       [ 0.44      ,  0.28571429,  0.25      ,  0.125     ,  0.57142857,\n",
       "         0.35369775,  1.        ],\n",
       "       [ 0.62      ,  0.60714286,  0.5       ,  0.5       ,  0.71428571,\n",
       "         0.45016077,  1.        ],\n",
       "       [ 0.36      ,  0.64285714,  0.75      ,  0.625     ,  0.42857143,\n",
       "         0.57877814,  0.        ],\n",
       "       [ 0.9       ,  0.82142857,  0.75      ,  0.875     ,  0.85714286,\n",
       "         0.92604502,  1.        ],\n",
       "       [ 0.74      ,  0.57142857,  1.        ,  1.        ,  0.57142857,\n",
       "         0.74919614,  1.        ],\n",
       "       [ 0.18      ,  0.5       ,  0.25      ,  0.75      ,  0.71428571,\n",
       "         0.51446945,  0.        ],\n",
       "       [ 0.62      ,  0.60714286,  0.75      ,  0.75      ,  0.71428571,\n",
       "         0.60450161,  1.        ],\n",
       "       [ 0.96      ,  0.82142857,  1.        ,  0.875     ,  1.        ,\n",
       "         0.78135048,  1.        ],\n",
       "       [ 0.48      ,  0.64285714,  0.5       ,  0.75      ,  0.71428571,\n",
       "         0.64308682,  0.        ],\n",
       "       [ 0.42      ,  0.42857143,  0.5       ,  0.875     ,  0.85714286,\n",
       "         0.52411576,  0.        ],\n",
       "       [ 0.86      ,  0.96428571,  1.        ,  1.        ,  0.85714286,\n",
       "         0.95819936,  1.        ],\n",
       "       [ 0.74      ,  0.42857143,  1.        ,  0.5       ,  0.57142857,\n",
       "         0.65594855,  1.        ],\n",
       "       [ 0.14      ,  0.21428571,  0.25      ,  0.375     ,  0.42857143,\n",
       "         0.27974277,  0.        ],\n",
       "       [ 0.12      ,  0.10714286,  0.25      ,  0.5       ,  0.14285714,\n",
       "         0.23794212,  1.        ],\n",
       "       [ 0.44      ,  0.39285714,  0.75      ,  0.75      ,  1.        ,\n",
       "         0.52411576,  0.        ],\n",
       "       [ 0.34      ,  0.32142857,  0.5       ,  0.75      ,  0.42857143,\n",
       "         0.45016077,  0.        ],\n",
       "       [ 0.46      ,  0.5       ,  0.25      ,  0.375     ,  0.14285714,\n",
       "         0.52411576,  0.        ],\n",
       "       [ 0.24      ,  0.25      ,  0.25      ,  0.        ,  0.14285714,\n",
       "         0.37620579,  0.        ],\n",
       "       [ 1.        ,  0.82142857,  1.        ,  0.875     ,  0.85714286,\n",
       "         0.85209003,  1.        ],\n",
       "       [ 1.        ,  0.71428571,  0.75      ,  1.        ,  0.85714286,\n",
       "         0.91961415,  1.        ],\n",
       "       [ 0.34      ,  0.46428571,  0.25      ,  0.375     ,  0.85714286,\n",
       "         0.4244373 ,  1.        ],\n",
       "       [ 0.2       ,  0.42857143,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.43729904,  0.        ],\n",
       "       [ 0.78      ,  0.75      ,  0.75      ,  0.75      ,  0.57142857,\n",
       "         0.82315113,  1.        ],\n",
       "       [ 0.6       ,  0.75      ,  0.25      ,  0.25      ,  0.28571429,\n",
       "         0.59163987,  1.        ],\n",
       "       [ 0.96      ,  0.92857143,  0.75      ,  0.5       ,  0.85714286,\n",
       "         0.83601286,  1.        ],\n",
       "       [ 0.82      ,  1.        ,  0.5       ,  0.75      ,  0.71428571,\n",
       "         0.69453376,  1.        ],\n",
       "       [ 0.36      ,  0.5       ,  0.5       ,  0.625     ,  0.28571429,\n",
       "         0.45337621,  1.        ],\n",
       "       [ 0.44      ,  0.53571429,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.59485531,  1.        ],\n",
       "       [ 0.2       ,  0.35714286,  0.5       ,  0.625     ,  0.28571429,\n",
       "         0.44051447,  0.        ],\n",
       "       [ 0.68      ,  0.78571429,  1.        ,  1.        ,  0.85714286,\n",
       "         0.73311897,  1.        ],\n",
       "       [ 0.7       ,  0.64285714,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.69453376,  1.        ],\n",
       "       [ 0.1       ,  0.25      ,  0.25      ,  0.375     ,  0.42857143,\n",
       "         0.2733119 ,  0.        ],\n",
       "       [ 0.4       ,  0.64285714,  0.        ,  0.125     ,  0.71428571,\n",
       "         0.13826367,  1.        ],\n",
       "       [ 0.36      ,  0.57142857,  0.75      ,  0.875     ,  1.        ,\n",
       "         0.49517685,  0.        ],\n",
       "       [ 0.54      ,  0.5       ,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.35048232,  1.        ],\n",
       "       [ 0.74      ,  0.75      ,  0.75      ,  0.875     ,  1.        ,\n",
       "         0.75241158,  0.        ],\n",
       "       [ 0.64      ,  0.64285714,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.6977492 ,  0.        ],\n",
       "       [ 0.54      ,  0.53571429,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.60450161,  1.        ],\n",
       "       [ 0.76      ,  0.64285714,  0.75      ,  0.75      ,  0.28571429,\n",
       "         0.71382637,  1.        ],\n",
       "       [ 0.72      ,  0.78571429,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.74276527,  1.        ],\n",
       "       [ 0.22      ,  0.25      ,  0.25      ,  0.5       ,  0.14285714,\n",
       "         0.45659164,  0.        ],\n",
       "       [ 1.        ,  1.        ,  0.75      ,  0.875     ,  0.71428571,\n",
       "         1.00321543,  1.        ],\n",
       "       [ 0.68      ,  0.67857143,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.75884244,  1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\7200\\AppData\\Local\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "## Hidden layer 1\n",
    "model.add(Dense(7, activation='relu', input_dim=7))\n",
    "\n",
    "## Hidden layer 2\n",
    "model.add(Dense(7, activation='relu', input_dim=7))\n",
    "\n",
    "## Output layer\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m8\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 18294.6797 - val_loss: 15618.5684\n",
      "Epoch 2/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14821.2559 - val_loss: 12575.0781\n",
      "Epoch 3/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11912.6104 - val_loss: 10081.9648\n",
      "Epoch 4/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9598.2354 - val_loss: 8066.2759\n",
      "Epoch 5/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7679.9043 - val_loss: 6446.7622\n",
      "Epoch 6/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6148.4780 - val_loss: 5146.2319\n",
      "Epoch 7/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4858.9570 - val_loss: 4099.8511\n",
      "Epoch 8/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3896.6355 - val_loss: 3380.9680\n",
      "Epoch 9/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3238.3330 - val_loss: 2803.2676\n",
      "Epoch 10/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2685.5156 - val_loss: 2343.4043\n",
      "Epoch 11/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2258.8892 - val_loss: 1964.2341\n",
      "Epoch 12/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1885.0382 - val_loss: 1648.9945\n",
      "Epoch 13/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1582.5310 - val_loss: 1386.0729\n",
      "Epoch 14/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1332.3247 - val_loss: 1165.0332\n",
      "Epoch 15/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1121.2131 - val_loss: 979.1359\n",
      "Epoch 16/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 938.4064 - val_loss: 822.3058\n",
      "Epoch 17/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 795.9345 - val_loss: 689.6927\n",
      "Epoch 18/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 660.1804 - val_loss: 577.6193\n",
      "Epoch 19/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 553.5355 - val_loss: 482.8629\n",
      "Epoch 20/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 465.3566 - val_loss: 402.7593\n",
      "Epoch 21/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 384.0901 - val_loss: 335.2015\n",
      "Epoch 22/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 322.0460 - val_loss: 278.1556\n",
      "Epoch 23/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 266.1793 - val_loss: 230.1519\n",
      "Epoch 24/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 221.1343 - val_loss: 189.8599\n",
      "Epoch 25/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 182.0878 - val_loss: 156.1481\n",
      "Epoch 26/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 148.8399 - val_loss: 127.9839\n",
      "Epoch 27/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 124.0344 - val_loss: 104.4845\n",
      "Epoch 28/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 100.6946 - val_loss: 85.0143\n",
      "Epoch 29/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 82.1295 - val_loss: 68.9321\n",
      "Epoch 30/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 66.8168 - val_loss: 55.7154\n",
      "Epoch 31/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 53.9942 - val_loss: 45.4334\n",
      "Epoch 32/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 43.7532 - val_loss: 38.3294\n",
      "Epoch 33/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 36.8686 - val_loss: 34.1002\n",
      "Epoch 34/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 33.6586 - val_loss: 31.7651\n",
      "Epoch 35/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 31.6667 - val_loss: 30.2780\n",
      "Epoch 36/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 30.0340 - val_loss: 29.1240\n",
      "Epoch 37/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 29.1860 - val_loss: 28.0838\n",
      "Epoch 38/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27.9529 - val_loss: 27.1169\n",
      "Epoch 39/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27.0276 - val_loss: 26.1916\n",
      "Epoch 40/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.9828 - val_loss: 25.2997\n",
      "Epoch 41/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.1164 - val_loss: 24.4344\n",
      "Epoch 42/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24.3264 - val_loss: 23.5972\n",
      "Epoch 43/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23.4774 - val_loss: 22.7861\n",
      "Epoch 44/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.6847 - val_loss: 21.9985\n",
      "Epoch 45/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.9954 - val_loss: 21.2354\n",
      "Epoch 46/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.1816 - val_loss: 20.4975\n",
      "Epoch 47/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.3995 - val_loss: 19.7810\n",
      "Epoch 48/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.7323 - val_loss: 19.0843\n",
      "Epoch 49/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.9518 - val_loss: 18.4091\n",
      "Epoch 50/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.2991 - val_loss: 17.7528\n",
      "Epoch 51/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.7017 - val_loss: 17.1188\n",
      "Epoch 52/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.9867 - val_loss: 16.5044\n",
      "Epoch 53/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.3901 - val_loss: 15.9082\n",
      "Epoch 54/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.8464 - val_loss: 15.3301\n",
      "Epoch 55/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.2550 - val_loss: 14.7700\n",
      "Epoch 56/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.7038 - val_loss: 14.2274\n",
      "Epoch 57/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.1622 - val_loss: 13.7026\n",
      "Epoch 58/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13.7061 - val_loss: 13.1932\n",
      "Epoch 59/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.1072 - val_loss: 12.7013\n",
      "Epoch 60/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.6729 - val_loss: 12.2244\n",
      "Epoch 61/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.2134 - val_loss: 11.7628\n",
      "Epoch 62/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.7624 - val_loss: 11.3161\n",
      "Epoch 63/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.2804 - val_loss: 10.8841\n",
      "Epoch 64/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.8536 - val_loss: 10.4665\n",
      "Epoch 65/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.3992 - val_loss: 10.0627\n",
      "Epoch 66/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.0375 - val_loss: 9.6715\n",
      "Epoch 67/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.6772 - val_loss: 9.2940\n",
      "Epoch 68/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.2281 - val_loss: 8.9297\n",
      "Epoch 69/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.9422 - val_loss: 8.5772\n",
      "Epoch 70/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.5578 - val_loss: 8.2369\n",
      "Epoch 71/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2106 - val_loss: 7.9084\n",
      "Epoch 72/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.8814 - val_loss: 7.5911\n",
      "Epoch 73/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5179 - val_loss: 7.2853\n",
      "Epoch 74/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.2588 - val_loss: 6.9898\n",
      "Epoch 75/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.9634 - val_loss: 6.7050\n",
      "Epoch 76/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7188 - val_loss: 6.4298\n",
      "Epoch 77/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.4322 - val_loss: 6.1648\n",
      "Epoch 78/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1635 - val_loss: 5.9091\n",
      "Epoch 79/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9248 - val_loss: 5.6630\n",
      "Epoch 80/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.6356 - val_loss: 5.4258\n",
      "Epoch 81/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.4396 - val_loss: 5.1971\n",
      "Epoch 82/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.2035 - val_loss: 4.9769\n",
      "Epoch 83/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.9306 - val_loss: 4.7651\n",
      "Epoch 84/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7417 - val_loss: 4.5612\n",
      "Epoch 85/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.5520 - val_loss: 4.3650\n",
      "Epoch 86/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3615 - val_loss: 4.1759\n",
      "Epoch 87/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.1583 - val_loss: 3.9941\n",
      "Epoch 88/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9979 - val_loss: 3.8193\n",
      "Epoch 89/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.8170 - val_loss: 3.6511\n",
      "Epoch 90/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6350 - val_loss: 3.4896\n",
      "Epoch 91/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.4962 - val_loss: 3.3341\n",
      "Epoch 92/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.3189 - val_loss: 3.1850\n",
      "Epoch 93/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.1812 - val_loss: 3.0416\n",
      "Epoch 94/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0284 - val_loss: 2.9040\n",
      "Epoch 95/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.9133 - val_loss: 2.7717\n",
      "Epoch 96/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8024 - val_loss: 2.6447\n",
      "Epoch 97/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6315 - val_loss: 2.5231\n",
      "Epoch 98/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5317 - val_loss: 2.4062\n",
      "Epoch 99/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.4063 - val_loss: 2.2942\n",
      "Epoch 100/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2890 - val_loss: 2.1868\n",
      "Epoch 101/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.1941 - val_loss: 2.0838\n",
      "Epoch 102/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.0999 - val_loss: 1.9850\n",
      "Epoch 103/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.9661 - val_loss: 1.8906\n",
      "Epoch 104/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8857 - val_loss: 1.8001\n",
      "Epoch 105/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.8240 - val_loss: 1.7132\n",
      "Epoch 106/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.7195 - val_loss: 1.6303\n",
      "Epoch 107/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.6405 - val_loss: 1.5509\n",
      "Epoch 108/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.5608 - val_loss: 1.4750\n",
      "Epoch 109/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4597 - val_loss: 1.4024\n",
      "Epoch 110/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4159 - val_loss: 1.3329\n",
      "Epoch 111/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3180 - val_loss: 1.2667\n",
      "Epoch 112/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2852 - val_loss: 1.2031\n",
      "Epoch 113/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2147 - val_loss: 1.1425\n",
      "Epoch 114/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1363 - val_loss: 1.0847\n",
      "Epoch 115/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0896 - val_loss: 1.0295\n",
      "Epoch 116/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0402 - val_loss: 0.9768\n",
      "Epoch 117/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9746 - val_loss: 0.9265\n",
      "Epoch 118/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9506 - val_loss: 0.8785\n",
      "Epoch 119/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8900 - val_loss: 0.8328\n",
      "Epoch 120/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8430 - val_loss: 0.7892\n",
      "Epoch 121/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7981 - val_loss: 0.7477\n",
      "Epoch 122/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7594 - val_loss: 0.7081\n",
      "Epoch 123/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7142 - val_loss: 0.6705\n",
      "Epoch 124/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6828 - val_loss: 0.6347\n",
      "Epoch 125/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6342 - val_loss: 0.6007\n",
      "Epoch 126/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6070 - val_loss: 0.5682\n",
      "Epoch 127/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5667 - val_loss: 0.5374\n",
      "Epoch 128/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5427 - val_loss: 0.5080\n",
      "Epoch 129/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5122 - val_loss: 0.4802\n",
      "Epoch 130/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4785 - val_loss: 0.4538\n",
      "Epoch 131/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4504 - val_loss: 0.4287\n",
      "Epoch 132/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4356 - val_loss: 0.4049\n",
      "Epoch 133/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4092 - val_loss: 0.3823\n",
      "Epoch 134/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3916 - val_loss: 0.3608\n",
      "Epoch 135/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3703 - val_loss: 0.3405\n",
      "Epoch 136/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3340 - val_loss: 0.3213\n",
      "Epoch 137/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3307 - val_loss: 0.3030\n",
      "Epoch 138/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3120 - val_loss: 0.2858\n",
      "Epoch 139/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2944 - val_loss: 0.2694\n",
      "Epoch 140/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2802 - val_loss: 0.2539\n",
      "Epoch 141/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2631 - val_loss: 0.2393\n",
      "Epoch 142/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2429 - val_loss: 0.2256\n",
      "Epoch 143/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2371 - val_loss: 0.2124\n",
      "Epoch 144/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2282 - val_loss: 0.2001\n",
      "Epoch 145/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2108 - val_loss: 0.1885\n",
      "Epoch 146/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1941 - val_loss: 0.1775\n",
      "Epoch 147/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1816 - val_loss: 0.1671\n",
      "Epoch 148/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1692 - val_loss: 0.1574\n",
      "Epoch 149/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1659 - val_loss: 0.1481\n",
      "Epoch 150/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1576 - val_loss: 0.1394\n",
      "Epoch 151/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1436 - val_loss: 0.1313\n",
      "Epoch 152/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1357 - val_loss: 0.1236\n",
      "Epoch 153/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1267 - val_loss: 0.1164\n",
      "Epoch 154/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1193 - val_loss: 0.1096\n",
      "Epoch 155/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1162 - val_loss: 0.1032\n",
      "Epoch 156/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1027 - val_loss: 0.0972\n",
      "Epoch 157/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1029 - val_loss: 0.0915\n",
      "Epoch 158/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0907 - val_loss: 0.0862\n",
      "Epoch 159/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0837 - val_loss: 0.0813\n",
      "Epoch 160/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0877 - val_loss: 0.0766\n",
      "Epoch 161/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0827 - val_loss: 0.0722\n",
      "Epoch 162/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0753 - val_loss: 0.0681\n",
      "Epoch 163/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0732 - val_loss: 0.0643\n",
      "Epoch 164/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0639 - val_loss: 0.0607\n",
      "Epoch 165/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0610 - val_loss: 0.0573\n",
      "Epoch 166/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0617 - val_loss: 0.0542\n",
      "Epoch 167/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0584 - val_loss: 0.0513\n",
      "Epoch 168/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0547 - val_loss: 0.0485\n",
      "Epoch 169/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0525 - val_loss: 0.0459\n",
      "Epoch 170/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0463 - val_loss: 0.0436\n",
      "Epoch 171/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0458 - val_loss: 0.0413\n",
      "Epoch 172/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0436 - val_loss: 0.0393\n",
      "Epoch 173/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0379 - val_loss: 0.0374\n",
      "Epoch 174/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0383 - val_loss: 0.0356\n",
      "Epoch 175/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0423 - val_loss: 0.0339\n",
      "Epoch 176/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0352 - val_loss: 0.0323\n",
      "Epoch 177/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0338 - val_loss: 0.0309\n",
      "Epoch 178/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0334 - val_loss: 0.0295\n",
      "Epoch 179/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0301 - val_loss: 0.0283\n",
      "Epoch 180/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0311 - val_loss: 0.0271\n",
      "Epoch 181/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0296 - val_loss: 0.0260\n",
      "Epoch 182/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0274 - val_loss: 0.0251\n",
      "Epoch 183/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0271 - val_loss: 0.0241\n",
      "Epoch 184/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0265 - val_loss: 0.0233\n",
      "Epoch 185/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0233 - val_loss: 0.0225\n",
      "Epoch 186/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0235 - val_loss: 0.0218\n",
      "Epoch 187/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0232 - val_loss: 0.0211\n",
      "Epoch 188/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0203 - val_loss: 0.0205\n",
      "Epoch 189/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0256 - val_loss: 0.0199\n",
      "Epoch 190/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0202 - val_loss: 0.0194\n",
      "Epoch 191/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0183 - val_loss: 0.0189\n",
      "Epoch 192/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0221 - val_loss: 0.0185\n",
      "Epoch 193/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0153 - val_loss: 0.0181\n",
      "Epoch 194/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0176 - val_loss: 0.0177\n",
      "Epoch 195/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0181 - val_loss: 0.0173\n",
      "Epoch 196/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0189 - val_loss: 0.0170\n",
      "Epoch 197/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0182 - val_loss: 0.0167\n",
      "Epoch 198/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0174 - val_loss: 0.0165\n",
      "Epoch 199/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0162 - val_loss: 0.0162\n",
      "Epoch 200/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 201/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0170 - val_loss: 0.0158\n",
      "Epoch 202/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0177 - val_loss: 0.0156\n",
      "Epoch 203/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0172 - val_loss: 0.0154\n",
      "Epoch 204/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0153 - val_loss: 0.0153\n",
      "Epoch 205/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0164 - val_loss: 0.0152\n",
      "Epoch 206/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0152 - val_loss: 0.0150\n",
      "Epoch 207/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0141 - val_loss: 0.0149\n",
      "Epoch 208/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0137 - val_loss: 0.0148\n",
      "Epoch 209/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0163 - val_loss: 0.0147\n",
      "Epoch 210/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0133 - val_loss: 0.0147\n",
      "Epoch 211/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0157 - val_loss: 0.0146\n",
      "Epoch 212/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0160 - val_loss: 0.0145\n",
      "Epoch 213/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0154 - val_loss: 0.0145\n",
      "Epoch 214/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0150 - val_loss: 0.0144\n",
      "Epoch 215/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0158 - val_loss: 0.0143\n",
      "Epoch 216/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0131 - val_loss: 0.0143\n",
      "Epoch 217/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0135 - val_loss: 0.0143\n",
      "Epoch 218/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0146 - val_loss: 0.0142\n",
      "Epoch 219/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0124 - val_loss: 0.0142\n",
      "Epoch 220/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0143 - val_loss: 0.0142\n",
      "Epoch 221/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0151 - val_loss: 0.0142\n",
      "Epoch 222/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0134 - val_loss: 0.0141\n",
      "Epoch 223/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0135 - val_loss: 0.0141\n",
      "Epoch 224/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 225/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0150 - val_loss: 0.0141\n",
      "Epoch 226/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0153 - val_loss: 0.0141\n",
      "Epoch 227/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0152 - val_loss: 0.0141\n",
      "Epoch 228/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0128 - val_loss: 0.0141\n",
      "Epoch 229/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0137 - val_loss: 0.0141\n",
      "Epoch 230/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 231/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0135 - val_loss: 0.0141\n",
      "Epoch 232/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0149 - val_loss: 0.0140\n",
      "Epoch 233/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0128 - val_loss: 0.0140\n",
      "Epoch 234/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0137 - val_loss: 0.0140\n",
      "Epoch 235/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0133 - val_loss: 0.0140\n",
      "Epoch 236/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0128 - val_loss: 0.0140\n",
      "Epoch 237/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0126 - val_loss: 0.0140\n",
      "Epoch 238/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0149 - val_loss: 0.0140\n",
      "Epoch 239/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 240/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0124 - val_loss: 0.0140\n",
      "Epoch 241/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 242/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0143 - val_loss: 0.0141\n",
      "Epoch 243/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 244/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0133 - val_loss: 0.0141\n",
      "Epoch 245/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0134 - val_loss: 0.0141\n",
      "Epoch 246/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0146 - val_loss: 0.0141\n",
      "Epoch 247/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0125 - val_loss: 0.0141\n",
      "Epoch 248/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0118 - val_loss: 0.0141\n",
      "Epoch 249/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0134 - val_loss: 0.0141\n",
      "Epoch 250/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0149 - val_loss: 0.0141\n",
      "Epoch 251/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 252/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0131 - val_loss: 0.0141\n",
      "Epoch 253/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0130 - val_loss: 0.0141\n",
      "Epoch 254/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 255/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0137 - val_loss: 0.0141\n",
      "Epoch 256/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0137 - val_loss: 0.0141\n",
      "Epoch 257/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 258/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0137 - val_loss: 0.0141\n",
      "Epoch 259/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0125 - val_loss: 0.0141\n",
      "Epoch 260/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0136 - val_loss: 0.0141\n",
      "Epoch 261/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0128 - val_loss: 0.0141\n",
      "Epoch 262/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0150 - val_loss: 0.0141\n",
      "Epoch 263/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0135 - val_loss: 0.0141\n",
      "Epoch 264/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0119 - val_loss: 0.0141\n",
      "Epoch 265/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0133 - val_loss: 0.0141\n",
      "Epoch 266/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 267/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0118 - val_loss: 0.0141\n",
      "Epoch 268/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 269/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0127 - val_loss: 0.0141\n",
      "Epoch 270/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0128 - val_loss: 0.0141\n",
      "Epoch 271/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0128 - val_loss: 0.0141\n",
      "Epoch 272/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0128 - val_loss: 0.0141\n",
      "Epoch 273/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0136 - val_loss: 0.0141\n",
      "Epoch 274/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 275/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0128 - val_loss: 0.0141\n",
      "Epoch 276/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 277/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0122 - val_loss: 0.0141\n",
      "Epoch 278/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0149 - val_loss: 0.0141\n",
      "Epoch 279/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0136 - val_loss: 0.0141\n",
      "Epoch 280/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0134 - val_loss: 0.0141\n",
      "Epoch 281/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0127 - val_loss: 0.0141\n",
      "Epoch 282/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0135 - val_loss: 0.0141\n",
      "Epoch 283/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0126 - val_loss: 0.0141\n",
      "Epoch 284/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 285/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0126 - val_loss: 0.0141\n",
      "Epoch 286/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0121 - val_loss: 0.0141\n",
      "Epoch 287/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0133 - val_loss: 0.0141\n",
      "Epoch 288/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 289/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0131 - val_loss: 0.0141\n",
      "Epoch 290/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0130 - val_loss: 0.0141\n",
      "Epoch 291/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0133 - val_loss: 0.0141\n",
      "Epoch 292/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0136 - val_loss: 0.0141\n",
      "Epoch 293/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0142 - val_loss: 0.0141\n",
      "Epoch 294/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0122 - val_loss: 0.0141\n",
      "Epoch 295/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0130 - val_loss: 0.0141\n",
      "Epoch 296/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0123 - val_loss: 0.0141\n",
      "Epoch 297/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0142 - val_loss: 0.0141\n",
      "Epoch 298/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0121 - val_loss: 0.0141\n",
      "Epoch 299/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 300/300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0134 - val_loss: 0.0141\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=300, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23cc37591b0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33610501839171825"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.014216508716344833"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
